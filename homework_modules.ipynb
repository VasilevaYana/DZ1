{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MeKai5Xj6eX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwlFrG-Tj6eY"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8BLmtZ3j6eZ"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKRkIjT8j6eZ"
      },
      "source": [
        "# Sequential container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb98PPpJj6ea"
      },
      "source": [
        "**Define** a forward and backward pass procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y2lav4dj6ea"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially.\n",
        "\n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "\n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "\n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})\n",
        "\n",
        "\n",
        "        Just write a little loop.\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.modules) == 0:\n",
        "            self.output = input\n",
        "            return self.output\n",
        "\n",
        "        current_input = input\n",
        "        for module in self.modules:\n",
        "            current_input = module.forward(current_input)\n",
        "\n",
        "        self.output = current_input\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "\n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)\n",
        "            gradInput = module[0].backward(input, g_1)\n",
        "\n",
        "\n",
        "        !!!\n",
        "\n",
        "        To ech module you need to provide the input, module saw while forward pass,\n",
        "        it is used while computing gradients.\n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
        "        and NOT `input` to this Sequential module.\n",
        "\n",
        "        !!!\n",
        "\n",
        "        \"\"\"\n",
        "        if len(self.modules) == 0:\n",
        "            self.gradInput = gradOutput\n",
        "            return self.gradInput\n",
        "\n",
        "        current_grad = gradOutput\n",
        "\n",
        "        for i in range(len(self.modules) - 1, 0, -1):\n",
        "            prev_output = self.modules[i-1].output\n",
        "            current_grad = self.modules[i].backward(prev_output, current_grad)\n",
        "\n",
        "        self.gradInput = self.modules[0].backward(input, current_grad)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "\n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfXdYfO4j6ea"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuwvBkuNj6ea"
      },
      "source": [
        "## 1 (0.2). Linear transform layer\n",
        "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0uoyqkpj6ea"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation\n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
        "\n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.dot(input, self.W.T) + self.b\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.dot(gradOutput, self.W)\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradW += np.dot(gradOutput.T, input)\n",
        "        self.gradb += np.sum(gradOutput, axis=0)\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNOnHXZJj6eb"
      },
      "source": [
        "## 2. (0.2) SoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VIValI0hj6eb"
      },
      "outputs": [],
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        exp_x = np.exp(self.output)\n",
        "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros_like(gradOutput)\n",
        "\n",
        "        for i in range(input.shape[0]):\n",
        "            softmax_output = self.output[i]\n",
        "            grad_output = gradOutput[i]\n",
        "\n",
        "            jacobian = np.diag(softmax_output) - np.outer(softmax_output, softmax_output)\n",
        "\n",
        "            self.gradInput[i] = np.dot(grad_output, jacobian)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy3DJjynj6eb"
      },
      "source": [
        "## 3. (0.2) LogSoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xo7DRdAJj6eb"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        exp_x = np.exp(self.output)\n",
        "        sum_exp = np.sum(exp_x, axis=1, keepdims=True)\n",
        "        self.output = self.output - np.log(sum_exp)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        softmax = np.exp(self.output)\n",
        "        self.gradInput = gradOutput - softmax * np.sum(gradOutput, axis=1, keepdims=True)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP5QdmmPj6eb"
      },
      "source": [
        "## 4. (0.3) Batch normalization\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fGTTDqVgj6eb"
      },
      "outputs": [],
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None\n",
        "        self.moving_variance = None\n",
        "        self.training = True\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # use self.EPS please\n",
        "        if self.training:\n",
        "            batch_mean = np.mean(input, axis=0)\n",
        "            batch_variance = np.var(input, axis=0)\n",
        "\n",
        "            if self.moving_mean is None:\n",
        "                self.moving_mean = batch_mean\n",
        "                self.moving_variance = batch_variance\n",
        "            else:\n",
        "                self.moving_mean = self.moving_mean * self.alpha + batch_mean * (1 - self.alpha)\n",
        "                self.moving_variance = self.moving_variance * self.alpha + batch_variance * (1 - self.alpha)\n",
        "\n",
        "            self.output = (input - batch_mean) / np.sqrt(batch_variance + self.EPS)\n",
        "        else:\n",
        "            self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        if self.training:\n",
        "            batch_size = input.shape[0]\n",
        "            batch_mean = np.mean(input, axis=0)\n",
        "            batch_variance = np.var(input, axis=0)\n",
        "\n",
        "            x_centered = input - batch_mean\n",
        "            std_inv = 1.0 / np.sqrt(batch_variance + self.EPS)\n",
        "\n",
        "            dx_norm = gradOutput\n",
        "            dvar = np.sum(dx_norm * x_centered, axis=0) * -0.5 * std_inv**3\n",
        "            dmean = np.sum(dx_norm * -std_inv, axis=0) + dvar * np.mean(-2.0 * x_centered, axis=0)\n",
        "\n",
        "            self.gradInput = dx_norm * std_inv + dvar * 2.0 * x_centered / batch_size + dmean / batch_size\n",
        "        else:\n",
        "            std_inv = 1.0 / np.sqrt(self.moving_variance + self.EPS)\n",
        "            self.gradInput = gradOutput * std_inv\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8XUS3Lt-j6eb"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5zjM3jj6eb"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gackeo1cj6eb"
      },
      "source": [
        "## 5. (0.3) Dropout\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmLQV3jXj6eb"
      },
      "outputs": [],
      "source": [
        "def updateOutput(self, input):\n",
        "    # Your code goes here. ################################################\n",
        "    if self.training:\n",
        "        self.mask = np.random.binomial(1, 1-self.p, input.shape)\n",
        "        scale = 1.0 / (1 - self.p)\n",
        "        self.output = input * self.mask * scale\n",
        "    else:\n",
        "        self.output = input\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, gradOutput):\n",
        "    # Your code goes here. ################################################\n",
        "    if self.training:\n",
        "        scale = 1.0 / (1 - self.p)\n",
        "        self.gradInput = gradOutput * self.mask * scale\n",
        "    else:\n",
        "        self.gradInput = gradOutput\n",
        "    return self.gradInput\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. (2.0) Conv2d\n",
        "Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."
      ],
      "metadata": {
        "id": "-WHGIqJFlhz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def updateOutput(self, input):\n",
        "    # Your code goes here. ################################################\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
        "\n",
        "    if not hasattr(self, 'weight'):\n",
        "        self.weight = torch.Tensor(self.out_channels, self.in_channels, *kernel_size)\n",
        "        self.weight.uniform_(-0.1, 0.1)\n",
        "\n",
        "    if self.bias and not hasattr(self, 'bias_term'):\n",
        "        self.bias_term = torch.Tensor(self.out_channels)\n",
        "        self.bias_term.uniform_(-0.1, 0.1)\n",
        "\n",
        "    self.output = F.conv2d(input, self.weight,\n",
        "                          bias=self.bias_term if self.bias else None,\n",
        "                          stride=self.stride,\n",
        "                          padding=self.padding,\n",
        "                          dilation=1,\n",
        "                          groups=1,\n",
        "                          padding_mode=self.padding_mode)\n",
        "\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, gradOutput):\n",
        "    # Your code goes here. ################################################\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
        "\n",
        "    weight_transposed = self.weight.transpose(0, 1).flip(2, 3)\n",
        "\n",
        "    self.gradInput = F.conv_transpose2d(gradOutput, weight_transposed,\n",
        "                                       stride=self.stride,\n",
        "                                       padding=self.padding,\n",
        "                                       output_padding=0,\n",
        "                                       groups=1,\n",
        "                                       dilation=1)\n",
        "\n",
        "    return self.gradInput\n"
      ],
      "metadata": {
        "id": "c1RjNoEXlOHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html). Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."
      ],
      "metadata": {
        "id": "updUVZE9qixP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def updateOutput(self, input):\n",
        "    # Your code goes here. ################################################\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
        "    stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
        "    padding = self.padding if isinstance(self.padding, tuple) else (self.padding, self.padding)\n",
        "\n",
        "    self.output = F.max_pool2d(input, kernel_size, stride, padding)\n",
        "\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, gradOutput):\n",
        "    # Your code goes here. ################################################\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
        "    stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
        "    padding = self.padding if isinstance(self.padding, tuple) else (self.padding, self.padding)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, indices = F.max_pool2d(input, kernel_size, stride, padding, return_indices=True)\n",
        "\n",
        "    self.gradInput = F.max_unpool2d(gradOutput, indices, kernel_size, stride, padding, input.shape)\n",
        "\n",
        "    return self.gradInput\n",
        "def updateOutput(self, input):\n",
        "    # Your code goes here. ################################################\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
        "    stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
        "    padding = self.padding if isinstance(self.padding, tuple) else (self.padding, self.padding)\n",
        "\n",
        "    self.output = F.avg_pool2d(input, kernel_size, stride, padding)\n",
        "\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, gradOutput):\n",
        "    # Your code goes here. ################################################\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    kernel_size = self.kernel_size if isinstance(self.kernel_size, tuple) else (self.kernel_size, self.kernel_size)\n",
        "    stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
        "    padding = self.padding if isinstance(self.padding, tuple) else (self.padding, self.padding)\n",
        "\n",
        "\n",
        "    batch_size, channels, height, width = input.shape\n",
        "    output_height = (height + 2 * padding[0] - kernel_size[0]) // stride[0] + 1\n",
        "    output_width = (width + 2 * padding[1] - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "\n",
        "    self.gradInput = torch.zeros_like(input)\n",
        "\n",
        "\n",
        "    ones = torch.ones(batch_size, channels, output_height, output_width, device=gradOutput.device)\n",
        "    kernel_elements = kernel_size[0] * kernel_size[1]\n",
        "\n",
        "    self.gradInput = F.fold(\n",
        "        F.unfold(gradOutput, (1, 1), stride=(1, 1), padding=(0, 0)) / kernel_elements,\n",
        "        (height, width),\n",
        "        kernel_size,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        dilation=1\n",
        "    )\n",
        "\n",
        "    return self.gradInput\n"
      ],
      "metadata": {
        "id": "Qys58EzkqhLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**. They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."
      ],
      "metadata": {
        "id": "KTN5R3CwrukV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GlobalMaxPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalMaxPool2d, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, channels, height, width = input.shape\n",
        "        return F.max_pool2d(input, kernel_size=(height, width))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}()\"\n",
        "\n",
        "\n",
        "class GlobalAvgPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalAvgPool2d, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, channels, height, width = input.shape\n",
        "        return F.avg_pool2d(input, kernel_size=(height, width))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}()\"\n",
        "\n",
        "\n",
        "def test_global_max_pool2d():\n",
        "    batch_size, channels, height, width = 2, 3, 4, 5\n",
        "    x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "    global_max_pool = GlobalMaxPool2d()\n",
        "\n",
        "    output = global_max_pool(x)\n",
        "\n",
        "    assert output.shape == (batch_size, channels, 1, 1), f\"Неверный размер выхода: {output.shape}\"\n",
        "\n",
        "    expected_output = torch.zeros(batch_size, channels, 1, 1)\n",
        "    for b in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            expected_output[b, c, 0, 0] = torch.max(x[b, c])\n",
        "\n",
        "    assert torch.allclose(output, expected_output), \"Результат GlobalMaxPool2d не соответствует ожидаемому\"\n",
        "\n",
        "    print(\"Тест GlobalMaxPool2d пройден успешно!\")\n",
        "\n",
        "\n",
        "def test_global_avg_pool2d():\n",
        "    batch_size, channels, height, width = 2, 3, 4, 5\n",
        "    x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "    global_avg_pool = GlobalAvgPool2d()\n",
        "\n",
        "    output = global_avg_pool(x)\n",
        "\n",
        "    assert output.shape == (batch_size, channels, 1, 1), f\"Неверный размер выхода: {output.shape}\"\n",
        "\n",
        "    expected_output = torch.zeros(batch_size, channels, 1, 1)\n",
        "    for b in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            expected_output[b, c, 0, 0] = torch.mean(x[b, c])\n",
        "\n",
        "    assert torch.allclose(output, expected_output), \"Результат GlobalAvgPool2d не соответствует ожидаемому\"\n",
        "\n",
        "    print(\"Тест GlobalAvgPool2d пройден успешно!\")\n",
        "\n",
        "\n",
        "test_global_max_pool2d()\n",
        "test_global_avg_pool2d()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqg8z11txhOS",
        "outputId": "26345fd0-9c77-49e8-df67-df58ff2494e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тест GlobalMaxPool2d пройден успешно!\n",
            "Тест GlobalAvgPool2d пройден успешно!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
      ],
      "metadata": {
        "id": "cYeBQDBhtViy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def updateOutput(self, input):\n",
        "    self.input_shape = input.shape\n",
        "    end_dim = self.end_dim if self.end_dim >= 0 else len(input.shape) + self.end_dim\n",
        "    new_shape = list(input.shape[:self.start_dim])\n",
        "    flattened_dim = 1\n",
        "    for i in range(self.start_dim, end_dim + 1):\n",
        "        flattened_dim *= input.shape[i]\n",
        "    new_shape.append(flattened_dim)\n",
        "    if end_dim + 1 < len(input.shape):\n",
        "        new_shape.extend(input.shape[end_dim + 1:])\n",
        "    self.output = input.reshape(new_shape)\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, gradOutput):\n",
        "    self.gradInput = gradOutput.reshape(self.input_shape)\n",
        "    return self.gradInput\n"
      ],
      "metadata": {
        "id": "SimPEMOFqhTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o36vPHSSj6eb"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_pryRQIj6ec"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sgm8bXjKj6ec"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB0UHGagj6ec"
      },
      "source": [
        "## 10. (0.1) Leaky ReLU\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agwfkwO0j6ec"
      },
      "outputs": [],
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope = 0.03):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "        self.slope = slope\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, self.slope * input)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.where(input > 0, 1, self.slope) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LeakyReLU\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-STyecvj6ec"
      },
      "source": [
        "## 11. (0.1) ELU\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jJSzEu1mj6ec"
      },
      "outputs": [],
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        super(ELU, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.where(input > 0, input, self.alpha * (np.exp(input) - 1))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.where(input > 0, 1, self.alpha * np.exp(input)) * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ELU\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn3C7KTqj6ec"
      },
      "source": [
        "## 12. (0.1) SoftPlus\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xcDPMssrj6ec"
      },
      "outputs": [],
      "source": [
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.log1p(np.exp(np.clip(input, -88, 88)))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        sigmoid_x = 1 / (1 + np.exp(-np.clip(input, -88, 88)))\n",
        "        self.gradInput = sigmoid_x * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. (0.2) Gelu\n",
        "Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."
      ],
      "metadata": {
        "id": "kw3PeZjOuo0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gelu(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        sqrt_2_over_pi = np.sqrt(2 / np.pi)\n",
        "        tanh_arg = sqrt_2_over_pi * (input + 0.044715 * np.power(input, 3))\n",
        "        self.output = input * 0.5 * (1 + np.tanh(tanh_arg))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        sqrt_2_over_pi = np.sqrt(2 / np.pi)\n",
        "        x_cubed = np.power(input, 3)\n",
        "        tanh_arg = sqrt_2_over_pi * (input + 0.044715 * x_cubed)\n",
        "        tanh_val = np.tanh(tanh_arg)\n",
        "        dtanh = 1 - np.power(tanh_val, 2)\n",
        "        darg = sqrt_2_over_pi * (1 + 0.044715 * 3 * np.power(input, 2))\n",
        "\n",
        "        dgelu = 0.5 * (1 + tanh_val) + 0.5 * input * dtanh * darg\n",
        "\n",
        "        self.gradInput = dgelu * gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Gelu\"\n"
      ],
      "metadata": {
        "id": "SdieE0Dtuo8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55p7UvPAj6ec"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NFaxZaqj6ec"
      },
      "source": [
        "Criterions are used to score the models answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGu45A8qj6ec"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuU26xkpj6ec"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i3VNuHhj6ec"
      },
      "outputs": [],
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LKLWNVj6ec"
      },
      "source": [
        "## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "die7KvW6j6ec"
      },
      "outputs": [],
      "source": [
        "def updateOutput(self, input, target):\n",
        "    # Use this trick to avoid numerical errors\n",
        "    input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "    # Your code goes here. ################################################\n",
        "    # Multiclass log loss: -sum(target * log(input)) / batch_size\n",
        "    log_probs = np.log(input_clamp)\n",
        "    batch_loss = -np.sum(target * log_probs)\n",
        "    self.output = batch_loss / input.shape[0]  # деление на размер батча\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, target):\n",
        "    # Use this trick to avoid numerical errors\n",
        "    input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "    # Your code goes here. ################################################\n",
        "    self.gradInput = -target / input_clamp / input.shape[0]\n",
        "    return self.gradInput\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHr_JbU5j6ec"
      },
      "source": [
        "## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v7N8bVP9j6ec"
      },
      "outputs": [],
      "source": [
        "def updateOutput(self, input, target):\n",
        "    # Your code goes here. ################################################\n",
        "    # Multiclass log loss: -sum(target * log(p)) / batch_size\n",
        "    batch_loss = -np.sum(target * input)\n",
        "    self.output = batch_loss / input.shape[0]\n",
        "    return self.output\n",
        "\n",
        "def updateGradInput(self, input, target):\n",
        "    # Your code goes here. ################################################\n",
        "    self.gradInput = -target / input.shape[0]\n",
        "    return self.gradInput\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E-ZnhKxaj6ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-я часть задания: реализация слоев, лосей и функций активации - 5 баллов. \\\\\n",
        "2-я часть задания: реализация моделей на своих классах. Что должно быть:\n",
        "  1. Выберите оптимизатор и реализуйте его, чтоб он работал с вами классами. - 1 балл.\n",
        "  2. Модель для задачи мультирегрессии на выбраных вами данных. Использовать FCNN, dropout, batchnorm, MSE. Пробуйте различные фукнции активации. Для первой модели попробуйте большую, среднюю и маленькую модель. - 1 балл.\n",
        "  3. Модель для задачи мультиклассификации на MNIST. Использовать свёртки, макспулы, флэттэны, софтмаксы - 1 балла.\n",
        "  4. Автоэнкодер для выбранных вами данных. Должен быть на свёртках и полносвязных слоях, дропаутах, батчнормах и тд. - 2 балла. \\\\\n",
        "\n",
        "Дополнительно в оценке каждой модели будет учитываться:\n",
        "1. Наличие правильно выбранной метрики и лосс функции.\n",
        "2. Отрисовка графиков лосей и метрик на трейне-валидации. Проверка качества модели на тесте.\n",
        "3. Наличие шедулера для lr.\n",
        "4. Наличие вормапа.\n",
        "5. Наличие механизма ранней остановки и сохранение лучшей модели.\n",
        "6. Свитч лося (метрики) и оптимайзера."
      ],
      "metadata": {
        "id": "TC2Bf1PP2Ios"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "IqMkzeEU0cqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class Module:\n",
        "    def __init__(self):\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        return self.updateGradInput(input, gradOutput)\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "    def gradParameters(self):\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for grad in self.gradParameters():\n",
        "            grad.fill(0)\n",
        "\n",
        "\n",
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(0, input)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * (input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "class LeakyReLU(Module):\n",
        "    def __init__(self, alpha=0.01):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.where(input > 0, input, input * self.alpha)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.where(input > 0, gradOutput, gradOutput * self.alpha)\n",
        "        return self.gradInput\n",
        "\n",
        "class Sigmoid(Module):\n",
        "    def __init__(self):\n",
        "        super(Sigmoid, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = 1 / (1 + np.exp(-input))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        sigmoid_output = self.output\n",
        "        self.gradInput = gradOutput * sigmoid_output * (1 - sigmoid_output)\n",
        "        return self.gradInput\n",
        "\n",
        "class Tanh(Module):\n",
        "    def __init__(self):\n",
        "        super(Tanh, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.tanh(input)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * (1 - np.square(self.output))\n",
        "        return self.gradInput\n",
        "\n",
        "class Softmax(Module):\n",
        "    def __init__(self, axis=-1):\n",
        "        super(Softmax, self).__init__()\n",
        "        self.axis = axis\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "\n",
        "        shifted_input = input - np.max(input, axis=self.axis, keepdims=True)\n",
        "        exp_values = np.exp(shifted_input)\n",
        "        self.output = exp_values / np.sum(exp_values, axis=self.axis, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "\n",
        "        self.gradInput = gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "class LogSoftmax(Module):\n",
        "    def __init__(self, axis=-1):\n",
        "        super(LogSoftmax, self).__init__()\n",
        "        self.axis = axis\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        shifted_input = input - np.max(input, axis=self.axis, keepdims=True)\n",
        "        exp_values = np.exp(shifted_input)\n",
        "        sum_exp = np.sum(exp_values, axis=self.axis, keepdims=True)\n",
        "        self.output = shifted_input - np.log(sum_exp)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        softmax_output = np.exp(self.output)\n",
        "        self.gradInput = gradOutput - np.sum(gradOutput * softmax_output, axis=self.axis, keepdims=True) * softmax_output\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "class Linear(Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        stdv = 1. / np.sqrt(in_features)\n",
        "        self.weight = np.random.uniform(-stdv, stdv, (out_features, in_features))\n",
        "        self.bias = np.random.uniform(-stdv, stdv, (out_features,))\n",
        "\n",
        "        self.gradWeight = np.zeros_like(self.weight)\n",
        "        self.gradBias = np.zeros_like(self.bias)\n",
        "\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.input_cache = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.input_cache = input\n",
        "        self.output = np.dot(input, self.weight.T) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.dot(gradOutput, self.weight)\n",
        "        self.gradWeight = np.dot(gradOutput.T, input)\n",
        "        self.gradBias = np.sum(gradOutput, axis=0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "    def gradParameters(self):\n",
        "        return [self.gradWeight, self.gradBias]\n",
        "\n",
        "class BatchNorm1d(Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super(BatchNorm1d, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        self.weight = np.ones((num_features,))\n",
        "        self.bias = np.zeros((num_features,))\n",
        "\n",
        "\n",
        "        self.gradWeight = np.zeros_like(self.weight)\n",
        "        self.gradBias = np.zeros_like(self.bias)\n",
        "\n",
        "\n",
        "        self.running_mean = np.zeros((num_features,))\n",
        "        self.running_var = np.ones((num_features,))\n",
        "\n",
        "\n",
        "        self.input_cache = None\n",
        "        self.norm_cache = None\n",
        "        self.std_cache = None\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        batch_size = input.shape[0]\n",
        "\n",
        "        if self.training:\n",
        "\n",
        "            batch_mean = np.mean(input, axis=0)\n",
        "            batch_var = np.var(input, axis=0)\n",
        "\n",
        "\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "\n",
        "            std = np.sqrt(batch_var + self.eps)\n",
        "            norm_input = (input - batch_mean) / std\n",
        "\n",
        "            self.input_cache = input\n",
        "            self.norm_cache = norm_input\n",
        "            self.std_cache = std\n",
        "        else:\n",
        "            norm_input = (input - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
        "\n",
        "        self.output = self.weight * norm_input + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        if not self.training:\n",
        "            self.gradInput = gradOutput * self.weight / np.sqrt(self.running_var + self.eps)\n",
        "            return self.gradInput\n",
        "\n",
        "        batch_size = input.shape[0]\n",
        "        norm_input = self.norm_cache\n",
        "        std = self.std_cache\n",
        "\n",
        "        self.gradWeight = np.sum(gradOutput * norm_input, axis=0)\n",
        "        self.gradBias = np.sum(gradOutput, axis=0)\n",
        "\n",
        "        dNorm = gradOutput * self.weight\n",
        "        dVar = np.sum(dNorm * (input - np.mean(input, axis=0)) * -0.5 * np.power(std, -3), axis=0)\n",
        "        dMean = np.sum(dNorm * -1 / std, axis=0) + dVar * np.mean(-2 * (input - np.mean(input, axis=0)), axis=0)\n",
        "\n",
        "        self.gradInput = dNorm / std + dVar * 2 * (input - np.mean(input, axis=0)) / batch_size + dMean / batch_size\n",
        "        return self.gradInput\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "    def gradParameters(self):\n",
        "        return [self.gradWeight, self.gradBias]\n",
        "\n",
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        if self.training:\n",
        "            self.mask = np.random.binomial(1, 1-self.p, input.shape) / (1-self.p)\n",
        "            self.output = input * self.mask\n",
        "        else:\n",
        "            self.output = input\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        if self.training:\n",
        "            self.gradInput = gradOutput * self.mask\n",
        "        else:\n",
        "            self.gradInput = gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "class Flatten(Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.input_shape = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.input_shape = input.shape\n",
        "        self.output = input.reshape(input.shape[0], -1)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput.reshape(self.input_shape)\n",
        "        return self.gradInput\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(Conv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
        "        self.padding = padding if isinstance(padding, tuple) else (padding, padding)\n",
        "\n",
        "        stdv = 1. / np.sqrt(in_channels * self.kernel_size[0] * self.kernel_size[1])\n",
        "        self.weight = np.random.uniform(-stdv, stdv, (out_channels, in_channels, self.kernel_size[0], self.kernel_size[1]))\n",
        "        self.bias = np.random.uniform(-stdv, stdv, (out_channels,))\n",
        "\n",
        "        self.gradWeight = np.zeros_like(self.weight)\n",
        "        self.gradBias = np.zeros_like(self.bias)\n",
        "\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.input_cache = None\n",
        "        self.input_padded = None\n",
        "\n",
        "    def _pad_input(self, input):\n",
        "        if self.padding[0] == 0 and self.padding[1] == 0:\n",
        "            return input\n",
        "\n",
        "        batch_size, channels, height, width = input.shape\n",
        "        padded_height = height + 2 * self.padding[0]\n",
        "        padded_width = width + 2 * self.padding[1]\n",
        "\n",
        "        padded = np.zeros((batch_size, channels, padded_height, padded_width))\n",
        "        padded[:, :, self.padding[0]:self.padding[0]+height, self.padding[1]:self.padding[1]+width] = input\n",
        "\n",
        "        return padded\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.input_cache = input\n",
        "        batch_size, channels, height, width = input.shape\n",
        "\n",
        "        self.input_padded = self._pad_input(input)\n",
        "        padded_height, padded_width = self.input_padded.shape[2], self.input_padded.shape[3]\n",
        "\n",
        "        output_height = (padded_height - self.kernel_size[0]) // self.stride[0] + 1\n",
        "        output_width = (padded_width - self.kernel_size[1]) // self.stride[1] + 1\n",
        "\n",
        "        self.output = np.zeros((batch_size, self.out_channels, output_height, output_width))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h_out in range(output_height):\n",
        "                    for w_out in range(output_width):\n",
        "                        h_start = h_out * self.stride[0]\n",
        "                        h_end = h_start + self.kernel_size[0]\n",
        "                        w_start = w_out * self.stride[1]\n",
        "                        w_end = w_start + self.kernel_size[1]\n",
        "\n",
        "                        patch = self.input_padded[b, :, h_start:h_end, w_start:w_end]\n",
        "                        self.output[b, c_out, h_out, w_out] = np.sum(patch * self.weight[c_out]) + self.bias[c_out]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        batch_size, channels, height, width = input.shape\n",
        "        padded_height, padded_width = self.input_padded.shape[2], self.input_padded.shape[3]\n",
        "\n",
        "        self.gradInput = np.zeros_like(self.input_padded)\n",
        "\n",
        "        output_height = gradOutput.shape[2]\n",
        "        output_width = gradOutput.shape[3]\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h_out in range(output_height):\n",
        "                    for w_out in range(output_width):\n",
        "                        h_start = h_out * self.stride[0]\n",
        "                        h_end = h_start + self.kernel_size[0]\n",
        "                        w_start = w_out * self.stride[1]\n",
        "                        w_end = w_start + self.kernel_size[1]\n",
        "\n",
        "                        patch = self.input_padded[b, :, h_start:h_end, w_start:w_end]\n",
        "                        self.gradWeight[c_out] += patch * gradOutput[b, c_out, h_out, w_out]\n",
        "\n",
        "                        self.gradInput[b, :, h_start:h_end, w_start:w_end] += self.weight[c_out] * gradOutput[b, c_out, h_out, w_out]\n",
        "\n",
        "        self.gradBias = np.sum(gradOutput, axis=(0, 2, 3))\n",
        "\n",
        "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
        "            self.gradInput = self.gradInput[:, :, self.padding[0]:self.padding[0]+height, self.padding[1]:self.padding[1]+width]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "    def gradParameters(self):\n",
        "        return [self.gradWeight, self.gradBias]\n",
        "\n",
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size, stride=None, padding=0):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.stride = stride if stride is not None else self.kernel_size\n",
        "        self.stride = self.stride if isinstance(self.stride, tuple) else (self.stride, self.stride)\n",
        "        self.padding = padding if isinstance(padding, tuple) else (padding, padding)\n",
        "\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.indices = None\n",
        "\n",
        "    def _pad_input(self, input):\n",
        "        if self.padding[0] == 0 and self.padding[1] == 0:\n",
        "            return input\n",
        "\n",
        "        batch_size, channels, height, width = input.shape\n",
        "        padded_height = height + 2 * self.padding[0]\n",
        "        padded_width = width + 2 * self.padding[1]\n",
        "\n",
        "        padded = np.zeros((batch_size, channels, padded_height, padded_width)) - np.inf\n",
        "        padded[:, :, self.padding[0]:self.padding[0]+height, self.padding[1]:self.padding[1]+width] = input\n",
        "\n",
        "        return padded\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        batch_size, channels, height, width = input.shape\n",
        "\n",
        "        input_padded = self._pad_input(input)\n",
        "        padded_height, padded_width = input_padded.shape[2], input_padded.shape[3]\n",
        "\n",
        "        output_height = (padded_height - self.kernel_size[0]) // self.stride[0] + 1\n",
        "        output_width = (padded_width - self.kernel_size[1]) // self.stride[1] + 1\n",
        "\n",
        "        self.output = np.zeros((batch_size, channels, output_height, output_width))\n",
        "        self.indices = np.zeros((batch_size, channels, output_height, output_width, 2), dtype=np.int32)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for h_out in range(output_height):\n",
        "                    for w_out in range(output_width):\n",
        "                        h_start = h_out * self.stride[0]\n",
        "                        h_end = h_start + self.kernel_size[0]\n",
        "                        w_start = w_out * self.stride[1]\n",
        "                        w_end = w_start + self.kernel_size[1]\n",
        "\n",
        "                        patch = input_padded[b, c, h_start:h_end, w_start:w_end]\n",
        "                        max_idx = np.unravel_index(np.argmax(patch), patch.shape)\n",
        "                        self.output[b, c, h_out, w_out] = patch[max_idx]\n",
        "                        self.indices[b, c, h_out, w_out] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        batch_size, channels, height, width = input.shape\n",
        "\n",
        "        self.gradInput = np.zeros((batch_size, channels, height + 2 * self.padding[0], width + 2 * self.padding[1]))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for h_out in range(gradOutput.shape[2]):\n",
        "                    for w_out in range(gradOutput.shape[3]):\n",
        "                        h_idx, w_idx = self.indices[b, c, h_out, w_out]\n",
        "                        self.gradInput[b, c, h_idx, w_idx] += gradOutput[b, c, h_out, w_out]\n",
        "\n",
        "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
        "            self.gradInput = self.gradInput[:, :, self.padding[0]:self.padding[0]+height, self.padding[1]:self.padding[1]+width]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "class MSELoss(Module):\n",
        "    def __init__(self):\n",
        "        super(MSELoss, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.mean(np.square(input - target))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput = 2 * (input - target) / input.size\n",
        "        return self.gradInput\n",
        "\n",
        "class CrossEntropyLoss(Module):\n",
        "    def __init__(self):\n",
        "        super(CrossEntropyLoss, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.logsoftmax = LogSoftmax()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        log_probs = self.logsoftmax.updateOutput(input)\n",
        "        self.output = -np.sum(target * log_probs) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        log_probs = self.logsoftmax.updateOutput(input)\n",
        "        self.gradInput = -target / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "class NLLLoss(Module):\n",
        "    def __init__(self):\n",
        "        super(NLLLoss, self).__init__()\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        batch_loss = -np.sum(target * input)\n",
        "        self.output = batch_loss / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput = -target / input.shape[0]\n",
        "        return self.gradInput\n",
        "class Sequential(Module):\n",
        "    def __init__(self, *args):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = list(args)\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def add(self, module):\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        current_input = input\n",
        "        for module in self.modules:\n",
        "            current_input = module.forward(current_input)\n",
        "        self.output = current_input\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        current_gradOutput = gradOutput\n",
        "        current_input = input\n",
        "\n",
        "        intermediate_outputs = [input]\n",
        "        for i, module in enumerate(self.modules[:-1]):\n",
        "            intermediate_outputs.append(module.forward(intermediate_outputs[-1]))\n",
        "        for i in range(len(self.modules) - 1, -1, -1):\n",
        "            module = self.modules[i]\n",
        "            current_input = intermediate_outputs[i]\n",
        "            current_gradOutput = module.backward(current_input, current_gradOutput)\n",
        "\n",
        "        self.gradInput = current_gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for module in self.modules:\n",
        "            params.extend(module.parameters())\n",
        "        return params\n",
        "\n",
        "    def gradParameters(self):\n",
        "        grad_params = []\n",
        "        for module in self.modules:\n",
        "            grad_params.extend(module.gradParameters())\n",
        "        return grad_params\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.eval()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for module in self.modules:\n",
        "            module.zero_grad()\n"
      ],
      "metadata": {
        "id": "Aq0X5QV55ZH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "g5RIi_gQ060z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, params, lr=0.01):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.fill(0)\n",
        "\n",
        "    def step(self):\n",
        "        pass\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, parameters, lr=0.01, momentum=0, weight_decay=0):\n",
        "        self.parameters = parameters\n",
        "        self.gradParameters = []\n",
        "        for param in parameters:\n",
        "            if isinstance(param, list):\n",
        "                self.gradParameters.extend(param)\n",
        "            else:\n",
        "                self.gradParameters.append(param)\n",
        "\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.velocity = [np.zeros_like(param) for param in self.gradParameters]\n",
        "\n",
        "    def step(self):\n",
        "        for i, (param, grad, vel) in enumerate(zip(self.parameters, self.gradParameters, self.velocity)):\n",
        "            if self.weight_decay != 0:\n",
        "                grad = grad + self.weight_decay * param\n",
        "\n",
        "            if self.momentum > 0:\n",
        "                vel *= self.momentum\n",
        "                vel += grad\n",
        "                param -= self.lr * vel\n",
        "            else:\n",
        "                param -= self.lr * grad\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    def __init__(self, parameters, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        self.parameters = parameters\n",
        "        self.gradParameters = []\n",
        "        for param in parameters:\n",
        "            if isinstance(param, list):\n",
        "                self.gradParameters.extend(param)\n",
        "            else:\n",
        "                self.gradParameters.append(param)\n",
        "\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "        self.m = [np.zeros_like(param) for param in self.gradParameters]\n",
        "        self.v = [np.zeros_like(param) for param in self.gradParameters]\n",
        "        self.t = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        beta1, beta2 = self.betas\n",
        "\n",
        "        for i, (param, grad, m, v) in enumerate(zip(self.parameters, self.gradParameters, self.m, self.v)):\n",
        "            if self.weight_decay != 0:\n",
        "                grad = grad + self.weight_decay * param\n",
        "            m *= beta1\n",
        "            m += (1 - beta1) * grad\n",
        "\n",
        "            v *= beta2\n",
        "            v += (1 - beta2) * (grad ** 2)\n",
        "\n",
        "            m_hat = m / (1 - beta1 ** self.t)\n",
        "            v_hat = v / (1 - beta2 ** self.t)\n",
        "\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "class LRScheduler:\n",
        "    def __init__(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.base_lr = optimizer.lr\n",
        "\n",
        "    def step(self):\n",
        "        pass\n",
        "\n",
        "class StepLR(LRScheduler):\n",
        "    def __init__(self, optimizer, step_size, gamma=0.1):\n",
        "        super(StepLR, self).__init__(optimizer)\n",
        "        self.step_size = step_size\n",
        "        self.gamma = gamma\n",
        "        self.epoch = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.epoch += 1\n",
        "        if self.epoch % self.step_size == 0:\n",
        "            self.optimizer.lr *= self.gamma\n",
        "\n",
        "class ReduceLROnPlateau(LRScheduler):\n",
        "    def __init__(self, optimizer, mode='min', factor=0.1, patience=10, threshold=1e-4, min_lr=0):\n",
        "        super(ReduceLROnPlateau, self).__init__(optimizer)\n",
        "        self.mode = mode\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.threshold = threshold\n",
        "        self.min_lr = min_lr\n",
        "\n",
        "        self.best = float('inf') if mode == 'min' else float('-inf')\n",
        "        self.num_bad_epochs = 0\n",
        "\n",
        "    def step(self, metrics):\n",
        "        current = metrics\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            is_better = current < self.best - self.threshold\n",
        "        else:\n",
        "            is_better = current > self.best + self.threshold\n",
        "\n",
        "        if is_better:\n",
        "            self.best = current\n",
        "            self.num_bad_epochs = 0\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs > self.patience:\n",
        "            self.optimizer.lr = max(self.optimizer.lr * self.factor, self.min_lr)\n",
        "            self.num_bad_epochs = 0\n",
        "class WarmupScheduler(LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_steps, initial_lr=0):\n",
        "        super(WarmupScheduler, self).__init__(optimizer)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.initial_lr = initial_lr\n",
        "        self.step_count = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_count += 1\n",
        "        if self.step_count <= self.warmup_steps:\n",
        "            progress = self.step_count / self.warmup_steps\n",
        "            self.optimizer.lr = self.initial_lr + progress * (self.base_lr - self.initial_lr)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0, mode='min'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = float('inf') if mode == 'min' else float('-inf')\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if self.mode == 'min':\n",
        "            is_better = score < self.best_score - self.min_delta\n",
        "        else:\n",
        "            is_better = score > self.best_score + self.min_delta\n",
        "\n",
        "        if is_better:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            self.best_model = self._save_model(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def _save_model(self, model):\n",
        "        model_copy = pickle.dumps(model)\n",
        "        return pickle.loads(model_copy)\n",
        "\n",
        "    def get_best_model(self):\n",
        "        return self.best_model\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.network = None\n",
        "        self.criterion = None\n",
        "        self.optimizer = None\n",
        "        self.scheduler = None\n",
        "        self.early_stopping = None\n",
        "        self.metrics = {}\n",
        "        self.history = {'train_loss': [], 'val_loss': [], 'train_metrics': {}, 'val_metrics': {}}\n",
        "\n",
        "    def compile(self, optimizer='sgd', loss='mse', metrics=None, lr=0.01, momentum=0, weight_decay=0):\n",
        "        if loss == 'mse':\n",
        "            self.criterion = MSELoss()\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.criterion = CrossEntropyLoss()\n",
        "        elif loss == 'nll':\n",
        "            self.criterion = NLLLoss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported loss function: {loss}\")\n",
        "\n",
        "        if optimizer == 'sgd':\n",
        "            self.optimizer = SGD(self.network.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        elif optimizer == 'adam':\n",
        "            self.optimizer = Adam(self.network.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
        "\n",
        "        if metrics:\n",
        "            self.metrics = {metric: [] for metric in metrics}\n",
        "\n",
        "    def set_scheduler(self, scheduler_type='step', **kwargs):\n",
        "        if scheduler_type == 'step':\n",
        "            self.scheduler = StepLR(self.optimizer, **kwargs)\n",
        "        elif scheduler_type == 'reduce_on_plateau':\n",
        "            self.scheduler = ReduceLROnPlateau(self.optimizer, **kwargs)\n",
        "        elif scheduler_type == 'warmup':\n",
        "            self.scheduler = WarmupScheduler(self.optimizer, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported scheduler: {scheduler_type}\")\n",
        "\n",
        "    def set_early_stopping(self, patience=7, min_delta=0, mode='min'):\n",
        "        self.early_stopping = EarlyStopping(patience=patience, min_delta=min_delta, mode=mode)\n",
        "\n",
        "    def train_step(self, inputs, targets):\n",
        "        outputs = self.network.forward(inputs)\n",
        "        loss = self.criterion.updateOutput(outputs, targets)\n",
        "        self.network.zero_grad()\n",
        "        grad_output = self.criterion.updateGradInput(outputs, targets)\n",
        "        self.network.backward(inputs, grad_output)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return outputs, loss\n",
        "\n",
        "    def evaluate(self, inputs, targets):\n",
        "        self.network.eval()\n",
        "        outputs = self.network.forward(inputs)\n",
        "        loss = self.criterion.updateOutput(outputs, targets)\n",
        "        self.network.train()\n",
        "\n",
        "        return outputs, loss\n",
        "\n",
        "    def calculate_metrics(self, outputs, targets, prefix=''):\n",
        "        results = {}\n",
        "\n",
        "        for metric_name in self.metrics:\n",
        "            if metric_name == 'accuracy':\n",
        "                if outputs.shape[1] > 1:\n",
        "                    predictions = np.argmax(outputs, axis=1)\n",
        "                    true_classes = np.argmax(targets, axis=1)\n",
        "                    accuracy = np.mean(predictions == true_classes)\n",
        "                else:\n",
        "                    predictions = (outputs > 0.5).astype(int)\n",
        "                    accuracy = np.mean(predictions == targets)\n",
        "\n",
        "                results[f'{prefix}accuracy'] = accuracy\n",
        "\n",
        "            elif metric_name == 'mse':\n",
        "                mse = np.mean(np.square(outputs - targets))\n",
        "                results[f'{prefix}mse'] = mse\n",
        "\n",
        "            elif metric_name == 'mae':\n",
        "                mae = np.mean(np.abs(outputs - targets))\n",
        "                results[f'{prefix}mae'] = mae\n",
        "\n",
        "        return results\n",
        "\n",
        "    def fit(self, train_inputs, train_targets, val_inputs=None, val_targets=None,\n",
        "            epochs=10, batch_size=32, verbose=1, callbacks=None):\n",
        "        n_samples = train_inputs.shape[0]\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            train_inputs_shuffled = train_inputs[indices]\n",
        "            train_targets_shuffled = train_targets[indices]\n",
        "\n",
        "            train_losses = []\n",
        "            train_metric_values = {metric: [] for metric in self.metrics}\n",
        "\n",
        "            self.network.train()\n",
        "\n",
        "            for batch_idx in tqdm(range(n_batches), disable=not verbose):\n",
        "                start_idx = batch_idx * batch_size\n",
        "                end_idx = min(start_idx + batch_size, n_samples)\n",
        "\n",
        "                batch_inputs = train_inputs_shuffled[start_idx:end_idx]\n",
        "                batch_targets = train_targets_shuffled[start_idx:end_idx]\n",
        "\n",
        "                outputs, loss = self.train_step(batch_inputs, batch_targets)\n",
        "                train_losses.append(loss)\n",
        "\n",
        "                if self.metrics:\n",
        "                    batch_metrics = self.calculate_metrics(outputs, batch_targets, prefix='train_')\n",
        "                    for metric_name, value in batch_metrics.items():\n",
        "                        train_metric_values[metric_name].append(value)\n",
        "\n",
        "            train_loss = np.mean(train_losses)\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "\n",
        "            for metric_name, values in train_metric_values.items():\n",
        "                if metric_name not in self.history['train_metrics']:\n",
        "                    self.history['train_metrics'][metric_name] = []\n",
        "                self.history['train_metrics'][metric_name].append(np.mean(values))\n",
        "\n",
        "            val_loss = None\n",
        "            if val_inputs is not None and val_targets is not None:\n",
        "                val_losses = []\n",
        "                val_metric_values = {metric: [] for metric in self.metrics}\n",
        "\n",
        "                self.network.eval()\n",
        "\n",
        "                n_val_samples = val_inputs.shape[0]\n",
        "                n_val_batches = (n_val_samples + batch_size - 1) // batch_size\n",
        "\n",
        "                for batch_idx in range(n_val_batches):\n",
        "                    start_idx = batch_idx * batch_size\n",
        "                    end_idx = min(start_idx + batch_size, n_val_samples)\n",
        "\n",
        "                    batch_inputs = val_inputs[start_idx:end_idx]\n",
        "                    batch_targets = val_targets[start_idx:end_idx]\n",
        "\n",
        "                    outputs, loss = self.evaluate(batch_inputs, batch_targets)\n",
        "                    val_losses.append(loss)\n",
        "\n",
        "                    if self.metrics:\n",
        "                        batch_metrics = self.calculate_metrics(outputs, batch_targets, prefix='val_')\n",
        "                        for metric_name, value in batch_metrics.items():\n",
        "                            val_metric_values[metric_name].append(value)\n",
        "\n",
        "                val_loss = np.mean(val_losses)\n",
        "                self.history['val_loss'].append(val_loss)\n",
        "\n",
        "                for metric_name, values in val_metric_values.items():\n",
        "                    if metric_name not in self.history['val_metrics']:\n",
        "                        self.history['val_metrics'][metric_name] = []\n",
        "                    self.history['val_metrics'][metric_name].append(np.mean(values))\n",
        "\n",
        "            if verbose:\n",
        "                log_str = f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\"\n",
        "\n",
        "                for metric_name, values in train_metric_values.items():\n",
        "                    log_str += f\", Train {metric_name}: {np.mean(values):.4f}\"\n",
        "\n",
        "                if val_loss is not None:\n",
        "                    log_str += f\", Val Loss: {val_loss:.4f}\"\n",
        "\n",
        "                    for metric_name, values in val_metric_values.items():\n",
        "                        log_str += f\", Val {metric_name}: {np.mean(values):.4f}\"\n",
        "\n",
        "                print(log_str)\n",
        "\n",
        "\n",
        "            if self.scheduler:\n",
        "                if isinstance(self.scheduler, ReduceLROnPlateau) and val_loss is not None:\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            if self.early_stopping and val_loss is not None:\n",
        "                if self.early_stopping(val_loss, self.network):\n",
        "                    print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "                    self.network = self.early_stopping.get_best_model()\n",
        "                    break\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def predict(self, inputs, batch_size=32):\n",
        "        n_samples = inputs.shape[0]\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        self.network.eval()\n",
        "\n",
        "        predictions = []\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(start_idx + batch_size, n_samples)\n",
        "\n",
        "            batch_inputs = inputs[start_idx:end_idx]\n",
        "            batch_outputs = self.network.forward(batch_inputs)\n",
        "            predictions.append(batch_outputs)\n",
        "\n",
        "        return np.vstack(predictions)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        with open(path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def plot_history(self, figsize=(12, 4)):\n",
        "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "        axes[0].plot(self.history['train_loss'], label='Train Loss')\n",
        "        if 'val_loss' in self.history and self.history['val_loss']:\n",
        "            axes[0].plot(self.history['val_loss'], label='Validation Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_title('Loss over epochs')\n",
        "        axes[0].legend()\n",
        "\n",
        "\n",
        "        if self.metrics:\n",
        "            for metric_name in self.history['train_metrics']:\n",
        "                axes[1].plot(self.history['train_metrics'][metric_name], label=f'Train {metric_name}')\n",
        "\n",
        "            for metric_name in self.history['val_metrics']:\n",
        "                axes[1].plot(self.history['val_metrics'][metric_name], label=f'Val {metric_name}')\n",
        "\n",
        "            axes[1].set_xlabel('Epoch')\n",
        "            axes[1].set_ylabel('Metric Value')\n",
        "            axes[1].set_title('Metrics over epochs')\n",
        "            axes[1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "class FCNNRegressionModel(Model):\n",
        "    def __init__(self, input_size, output_size, hidden_sizes=[64, 32], dropout_rate=0.2, use_batchnorm=True):\n",
        "        super(FCNNRegressionModel, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        layers.append(Linear(input_size, hidden_sizes[0]))\n",
        "        if use_batchnorm:\n",
        "            layers.append(BatchNorm1d(hidden_sizes[0]))\n",
        "        layers.append(ReLU())\n",
        "        layers.append(Dropout(dropout_rate))\n",
        "\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            layers.append(Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            if use_batchnorm:\n",
        "                layers.append(BatchNorm1d(hidden_sizes[i+1]))\n",
        "            layers.append(ReLU())\n",
        "            layers.append(Dropout(dropout_rate))\n",
        "\n",
        "        layers.append(Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "        self.network = Sequential(*layers)\n",
        "\n",
        "    def set_activation(self, activation_type):\n",
        "        for i, module in enumerate(self.network.modules):\n",
        "            if isinstance(module, ReLU) or isinstance(module, LeakyReLU) or isinstance(module, Sigmoid) or isinstance(module, Tanh):\n",
        "                if activation_type == 'relu':\n",
        "                    self.network.modules[i] = ReLU()\n",
        "                elif activation_type == 'leaky_relu':\n",
        "                    self.network.modules[i] = LeakyReLU()\n",
        "                elif activation_type == 'sigmoid':\n",
        "                    self.network.modules[i] = Sigmoid()\n",
        "                elif activation_type == 'tanh':\n",
        "                    self.network.modules[i] = Tanh()\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported activation function: {activation_type}\")\n",
        "\n",
        "class CNNClassificationModel(Model):\n",
        "    def __init__(self, input_channels=1, num_classes=10):\n",
        "        super(CNNClassificationModel, self).__init__()\n",
        "\n",
        "        self.network = Sequential(\n",
        "            Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Flatten(),\n",
        "            Linear(128 * 3 * 3, 128),\n",
        "            ReLU(),\n",
        "            Dropout(0.5),\n",
        "            Linear(128, num_classes),\n",
        "            Softmax()\n",
        "        )\n",
        "\n",
        "class ConvAutoencoder(Model):\n",
        "    def __init__(self, input_channels=1, latent_dim=8):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder = Sequential(\n",
        "            Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
        "            BatchNorm1d(32),\n",
        "            ReLU(),\n",
        "\n",
        "            Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            BatchNorm1d(64),\n",
        "            ReLU(),\n",
        "\n",
        "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            BatchNorm1d(128),\n",
        "            ReLU(),\n",
        "\n",
        "            Flatten(),\n",
        "            Linear(128 * 4 * 4, latent_dim),\n",
        "            BatchNorm1d(latent_dim),\n",
        "            Tanh()\n",
        "        )\n",
        "\n",
        "        self.decoder = Sequential(\n",
        "            Linear(latent_dim, 128 * 4 * 4),\n",
        "            BatchNorm1d(128 * 4 * 4),\n",
        "            ReLU(),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.network = Sequential(\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "1HsLCzrU071S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}